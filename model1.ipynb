{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Conv2D, MaxPool2D, Lambda, Bidirectional, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import squeeze\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe meme_data and drop duplicates\n",
    "df = pd.read_csv('dataset/memes_data.tsv', sep='\\t')\n",
    "df1 = df.drop_duplicates(subset=['HashId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe as long as the amount of images downloaded locally\n",
    "# total dataset is >200000, I have only downloaded 113914 memes\n",
    "df1 = df1.loc[:114517] # the dataframe index goes up to 114517 from original dataframe since we dropped duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate empty list to append all images into an numpy array\n",
    "images_as_array = []\n",
    "\n",
    "# for loop to preprocess images, changing all image sizes to 64x64 and converting to grayscale to reduce the amount of data\n",
    "for i in range(len(df1)):\n",
    "    try:\n",
    "        if len(df1['CaptionText'].loc[i]) <= 128:\n",
    "            image = tf.keras.preprocessing.image.load_img(f'D:/lh_final_data_images/img{str(i)}.jpg', color_mode='grayscale', target_size=(64, 64))\n",
    "            input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "            images_as_array.append(input_arr)\n",
    "        else:\n",
    "            images_as_array.append(np.nan)\n",
    "    except:\n",
    "        images_as_array.append(np.nan) # this is to append nan values where images did not download correctly/are corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows that all images were converted to numpy array\n",
    "len(images_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking shape of a single numpy array image\n",
    "images_as_array[242].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the numpy array of images to dataframe\n",
    "df1['img_array'] = images_as_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AltText           0\n",
       "CaptionText       0\n",
       "ImageURL          0\n",
       "HashId            0\n",
       "MemeLabel         0\n",
       "img_array      8815\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the null values we got from images that were corrupted\n",
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AltText        0\n",
       "CaptionText    0\n",
       "ImageURL       0\n",
       "HashId         0\n",
       "MemeLabel      0\n",
       "img_array      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b786d16fda66>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1['CaptionText'] = df1.CaptionText.str.replace('[^a-zA-Z0-9]', ' ')\n"
     ]
    }
   ],
   "source": [
    "# code to keep only alphanumeric characters in the labels column\n",
    "df1['CaptionText'] = df1.CaptionText.str.replace('[^a-zA-Z0-9]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace that is present at start and end of every entry\n",
    "df1['CaptionText'] = df1['CaptionText'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking histogram of character length\n",
    "h = sorted(df1['CaptionText'].str.len().values)\n",
    "import pylab as plt\n",
    "plt.hist(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram shows us that most memes are under 150 characters. I will use 128 as it will be easier to shape later in our model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105099, 128)\n"
     ]
    }
   ],
   "source": [
    "# with max length=128, transform each row to a list of 128 integer values using tokenization\n",
    "maxlen = 128\n",
    "t = Tokenizer(char_level=True, lower=False)\n",
    "t.fit_on_texts(df1['CaptionText'])\n",
    "tokenized = t.texts_to_sequences(df1['CaptionText'])\n",
    "text_labels = preprocessing.sequence.pad_sequences(tokenized, maxlen=maxlen, padding='post')\n",
    "print(text_labels.shape) # check shape of our padded, numerical text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 36,  1, 35, 16, 35,  1, 41, 30, 10, 20,  1, 22,  1, 40, 31, 19,\n",
       "       36,  1, 41, 22, 18, 30,  1, 35, 36,  1, 40, 30, 16, 20, 10,  1, 42,\n",
       "       16, 26,  1, 55, 46,  1, 35, 22, 20, 32, 18, 10, 25,  1, 35, 36,  1,\n",
       "       35, 16, 35,  1, 41, 30, 10, 20,  1, 25, 30, 10,  1, 31, 16, 16, 44,\n",
       "       25,  1, 19, 18,  1, 22, 20, 25, 18, 19, 38, 26, 19, 35,  1, 40, 30,\n",
       "       16, 20, 10,  1, 42, 16, 26,  1, 18, 41, 16,  1, 30, 16, 32, 26, 25,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_labels[0] # checking how a single label looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_labels = text_labels.astype(np.int) # change labels to int type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'E': 2, 'O': 3, 'T': 4, 'A': 5, 'I': 6, 'N': 7, 'S': 8, 'R': 9, 'e': 10, 'H': 11, 'L': 12, 'M': 13, 'D': 14, 'U': 15, 'o': 16, 'Y': 17, 't': 18, 'a': 19, 'n': 20, 'G': 21, 'i': 22, 'C': 23, 'W': 24, 's': 25, 'r': 26, 'P': 27, 'F': 28, 'B': 29, 'h': 30, 'l': 31, 'u': 32, 'K': 33, 'd': 34, 'm': 35, 'y': 36, 'V': 37, 'g': 38, 'c': 39, 'p': 40, 'w': 41, 'f': 42, 'b': 43, 'k': 44, 'v': 45, '0': 46, 'J': 47, '1': 48, 'X': 49, '2': 50, 'Z': 51, '9': 52, '5': 53, 'x': 54, '3': 55, 'Q': 56, 'j': 57, '4': 58, 'z': 59, '6': 60, '8': 61, '7': 62, 'q': 63}\n"
     ]
    }
   ],
   "source": [
    "# check to see what our characters consist of (all alphanumeric characters plus a whitespace character)\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only label and input data into new dataframe\n",
    "new_df = df1[['CaptionText', 'img_array']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaptionText</th>\n",
       "      <th>img_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my mom when i play with my phone for 30 minute...</td>\n",
       "      <td>[[[150.0], [152.0], [152.0], [152.0], [154.0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doing your own research for a test Copy and pa...</td>\n",
       "      <td>[[[148.0], [151.0], [152.0], [153.0], [154.0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 000 000 followers on tik tok 1 point on imgflip</td>\n",
       "      <td>[[[149.0], [151.0], [152.0], [153.0], [154.0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Making original memes Following the idea of an...</td>\n",
       "      <td>[[[148.0], [151.0], [152.0], [153.0], [154.0],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>[[[149.0], [151.0], [152.0], [153.0], [154.0],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         CaptionText  \\\n",
       "0  my mom when i play with my phone for 30 minute...   \n",
       "1  Doing your own research for a test Copy and pa...   \n",
       "2  1 000 000 followers on tik tok 1 point on imgflip   \n",
       "3  Making original memes Following the idea of an...   \n",
       "4                                                  M   \n",
       "\n",
       "                                           img_array  \n",
       "0  [[[150.0], [152.0], [152.0], [152.0], [154.0],...  \n",
       "1  [[[148.0], [151.0], [152.0], [153.0], [154.0],...  \n",
       "2  [[[149.0], [151.0], [152.0], [153.0], [154.0],...  \n",
       "3  [[[148.0], [151.0], [152.0], [153.0], [154.0],...  \n",
       "4  [[[149.0], [151.0], [152.0], [153.0], [154.0],...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all corrupted images from our preprocessed image arrays\n",
    "cleanX = [x for x in images_as_array if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide all values by 255 for normalization between 0 and 1\n",
    "cleanX1 = []\n",
    "for i in cleanX:\n",
    "    cleanX1.append(i / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of a single image array\n",
    "cleanX1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the array to fit into our model as it will require tensor shape\n",
    "cleanX1 = np.stack(cleanX1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105099, 64, 64, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanX1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will consist of a convolution part with 6 convolution layers and 3 max pooling layers. Batch normalization layers will also be applied here for ease of data processing for our model.\\\n",
    "The convolution layer will then be connected end-to-end to a recurrent part through a flatten and reshape layer.\\\n",
    "There will be two bidirectional LSTM layers that will process the information picked up by the convolution part.\\\n",
    "This is be fed to a dense layer for 64 nodes, a node for each alphanumeric character to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Reshape((128, 512), input_shape=(65536,)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)))\n",
    "\n",
    "model.add(Dense(64, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 16, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 16, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 16, 512)        2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 256)          656384    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 64)           16448     \n",
      "=================================================================\n",
      "Total params: 6,750,528\n",
      "Trainable params: 6,748,992\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1314/1314 [==============================] - 140s 100ms/step - loss: 1.8183 - accuracy: 0.5860 - val_loss: 1.8916 - val_accuracy: 0.5599\n",
      "Epoch 2/25\n",
      "1314/1314 [==============================] - 134s 102ms/step - loss: 1.7882 - accuracy: 0.5894 - val_loss: 1.9028 - val_accuracy: 0.5599\n",
      "Epoch 3/25\n",
      "1314/1314 [==============================] - 133s 101ms/step - loss: 1.7830 - accuracy: 0.5902 - val_loss: 1.9949 - val_accuracy: 0.5599\n",
      "Epoch 4/25\n",
      "1314/1314 [==============================] - 134s 102ms/step - loss: 1.7799 - accuracy: 0.5905 - val_loss: 1.9173 - val_accuracy: 0.5573\n",
      "Epoch 5/25\n",
      "1314/1314 [==============================] - 134s 102ms/step - loss: 1.7781 - accuracy: 0.5907 - val_loss: 1.9056 - val_accuracy: 0.5585\n",
      "Epoch 6/25\n",
      "1314/1314 [==============================] - 131s 100ms/step - loss: 1.7765 - accuracy: 0.5908 - val_loss: 1.9367 - val_accuracy: 0.5608\n",
      "Epoch 7/25\n",
      "1314/1314 [==============================] - 133s 102ms/step - loss: 1.7753 - accuracy: 0.5909 - val_loss: 1.9138 - val_accuracy: 0.5585\n",
      "Epoch 8/25\n",
      "1314/1314 [==============================] - 134s 102ms/step - loss: 1.7746 - accuracy: 0.5909 - val_loss: 1.9262 - val_accuracy: 0.5590\n",
      "Epoch 9/25\n",
      "1314/1314 [==============================] - 131s 100ms/step - loss: 1.7739 - accuracy: 0.5910 - val_loss: 1.9923 - val_accuracy: 0.5601\n",
      "Epoch 10/25\n",
      "1314/1314 [==============================] - 128s 98ms/step - loss: 1.7734 - accuracy: 0.5910 - val_loss: 1.8906 - val_accuracy: 0.5597\n",
      "Epoch 11/25\n",
      "1314/1314 [==============================] - 120s 91ms/step - loss: 1.7725 - accuracy: 0.5911 - val_loss: 1.8907 - val_accuracy: 0.5600\n",
      "Epoch 12/25\n",
      "1314/1314 [==============================] - 119s 90ms/step - loss: 1.7719 - accuracy: 0.5912 - val_loss: 1.8925 - val_accuracy: 0.5594\n",
      "Epoch 13/25\n",
      "1314/1314 [==============================] - 122s 93ms/step - loss: 1.7711 - accuracy: 0.5912 - val_loss: 1.9251 - val_accuracy: 0.5570\n",
      "Epoch 14/25\n",
      "1314/1314 [==============================] - 119s 91ms/step - loss: 1.7707 - accuracy: 0.5912 - val_loss: 1.9183 - val_accuracy: 0.5589\n",
      "Epoch 15/25\n",
      "1314/1314 [==============================] - 118s 90ms/step - loss: 1.7703 - accuracy: 0.5913 - val_loss: 1.9340 - val_accuracy: 0.5606\n",
      "Epoch 16/25\n",
      "1314/1314 [==============================] - 117s 89ms/step - loss: 1.7695 - accuracy: 0.5913 - val_loss: 1.9219 - val_accuracy: 0.5593\n",
      "Epoch 17/25\n",
      "1314/1314 [==============================] - 119s 91ms/step - loss: 1.7690 - accuracy: 0.5914 - val_loss: 1.9013 - val_accuracy: 0.5598\n",
      "Epoch 18/25\n",
      "1314/1314 [==============================] - 117s 89ms/step - loss: 1.7684 - accuracy: 0.5914 - val_loss: 1.9521 - val_accuracy: 0.5590\n",
      "Epoch 19/25\n",
      "1314/1314 [==============================] - 118s 90ms/step - loss: 1.7676 - accuracy: 0.5914 - val_loss: 1.9041 - val_accuracy: 0.5587\n",
      "Epoch 20/25\n",
      "1314/1314 [==============================] - 113s 86ms/step - loss: 1.7669 - accuracy: 0.5915 - val_loss: 1.8966 - val_accuracy: 0.5584\n",
      "Epoch 21/25\n",
      "1314/1314 [==============================] - 114s 87ms/step - loss: 1.7663 - accuracy: 0.5916 - val_loss: 1.9083 - val_accuracy: 0.5588\n",
      "Epoch 22/25\n",
      "1314/1314 [==============================] - 118s 90ms/step - loss: 1.7656 - accuracy: 0.5916 - val_loss: 1.9086 - val_accuracy: 0.5584\n",
      "Epoch 23/25\n",
      "1314/1314 [==============================] - 120s 91ms/step - loss: 1.7651 - accuracy: 0.5916 - val_loss: 1.9045 - val_accuracy: 0.5596\n",
      "Epoch 24/25\n",
      "1314/1314 [==============================] - 117s 89ms/step - loss: 1.7645 - accuracy: 0.5917 - val_loss: 1.9053 - val_accuracy: 0.5590\n",
      "Epoch 25/25\n",
      "1314/1314 [==============================] - 116s 88ms/step - loss: 1.7638 - accuracy: 0.5917 - val_loss: 1.9244 - val_accuracy: 0.5606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x259ceed3a30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=cleanX1, y=int_labels, batch_size=64, epochs=25, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "loaded_model = keras.models.load_model('model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
